{
     "parent1": {
          "algorithm": "\nCreate a new algorithm that updates the distance matrix by considering the weighted sum of the logarithm of normalized edge usage, the squared difference between the local and global optimum tours, and a random factor, and sorts the nodes based on the minimum distance from the global optimum tour and maximum edge usage to determine the top nodes for perturbation.\n",
          "code": "import numpy as np\n\ndef get_matrix_and_nodes(distance_matrix, local_opt_tour, global_opt_tour, edge_n_used):\n    weighted_combined_metric = (0.3 * (np.log(edge_n_used / np.max(edge_n_used)))) + (0.7 * (local_opt_tour - global_opt_tour) ** 2)\n    randomness = np.random.rand(distance_matrix.shape[0], distance_matrix.shape[1])\n    new_distance_matrix = distance_matrix + weighted_combined_metric + (0.2 * randomness)\n    perturb_nodes = np.argsort(np.maximum(np.max(edge_n_used, axis=1), np.min(new_distance_matrix, axis=1)))\n    \n    return new_distance_matrix, perturb_nodes",
          "objective": 0.03099,
          "other_inf": null
     },
     "parent2": {
          "algorithm": "\nCreate a new algorithm that updates the distance matrix by computing a combination of the normalized edge usage, the weighted average of the difference between local and global optimum tours, and additional randomness with parameters (0.3, 0.5, 0.2) respectively, then sorts the nodes based on the maximum normalized edge usage and minimum distance from the global optimum tour to determine the top nodes for perturbation.\n",
          "code": "import numpy as np\n\ndef get_matrix_and_nodes(distance_matrix, local_opt_tour, global_opt_tour, edge_n_used):\n    combined_metric = (0.3 * (edge_n_used / np.max(edge_n_used))) + (0.5 * (local_opt_tour - global_opt_tour)) + (0.2 * np.random.rand(distance_matrix.shape[0], distance_matrix.shape[1]))\n    new_distance_matrix = distance_matrix + combined_metric\n    perturb_nodes = np.argsort(np.maximum(np.max(edge_n_used, axis=1), np.min(new_distance_matrix, axis=1)))\n    \n    return new_distance_matrix, perturb_nodes",
          "objective": 0.03929,
          "other_inf": null
     },
     "parent3": {
          "algorithm": "new_distance_matrix, perturb_nodes",
          "code": "import numpy as np\n\ndef get_matrix_and_nodes(distance_matrix, local_opt_tour, global_opt_tour, edge_n_used):\n    tour_difference = (local_opt_tour - global_opt_tour) ** 2\n    average_edge_usage = np.mean(edge_n_used, axis=1)\n    weighted_factor = 0.5 \n    inverse_distance_matrix = 1 / (distance_matrix + 1)\n    \n    new_distance_matrix = distance_matrix + (tour_difference + average_edge_usage * np.random.rand(distance_matrix.shape[0]) + inverse_distance_matrix) * weighted_factor\n    perturb_nodes = np.argsort(np.maximum(np.max(edge_n_used, axis=1), np.max(new_distance_matrix, axis=1)))\n    \n    return new_distance_matrix, perturb_nodes",
          "objective": 0.04594,
          "other_inf": null
     },
     "parent4": {
          "algorithm": "new_distance_matrix, perturb_nodes",
          "code": "import numpy as np\n\ndef get_matrix_and_nodes(distance_matrix, local_opt_tour, global_opt_tour, edge_n_used):\n    tour_difference = (local_opt_tour - global_opt_tour) ** 2\n    average_edge_usage = np.mean(edge_n_used, axis=1)\n    weighted_factor = 0.5 \n    inverse_distance_matrix = 1 / (distance_matrix + 1)\n    \n    new_distance_matrix = distance_matrix + (tour_difference + average_edge_usage * np.random.rand(distance_matrix.shape[0]) + inverse_distance_matrix) * weighted_factor\n    perturb_nodes = np.argsort(np.maximum(np.max(edge_n_used, axis=1), np.max(new_distance_matrix, axis=1)))\n    \n    return new_distance_matrix, perturb_nodes",
          "objective": 0.04594,
          "other_inf": null
     },
     "parent5": {
          "algorithm": "\nCreate a new algorithm that updates the distance matrix by computing a combination of the normalized edge usage, the weighted average of the difference between local and global optimum tours, and additional randomness with parameters (0.3, 0.5, 0.2) respectively, then sorts the nodes based on the maximum normalized edge usage and minimum distance from the global optimum tour to determine the top nodes for perturbation.\n",
          "code": "import numpy as np\n\ndef get_matrix_and_nodes(distance_matrix, local_opt_tour, global_opt_tour, edge_n_used):\n    combined_metric = (0.3 * (edge_n_used / np.max(edge_n_used))) + (0.5 * (local_opt_tour - global_opt_tour)) + (0.2 * np.random.rand(distance_matrix.shape[0], distance_matrix.shape[1]))\n    new_distance_matrix = distance_matrix + combined_metric\n    perturb_nodes = np.argsort(np.maximum(np.max(edge_n_used, axis=1), np.min(new_distance_matrix, axis=1)))\n    \n    return new_distance_matrix, perturb_nodes",
          "objective": 0.03929,
          "other_inf": null
     },
     "offspring": {
          "algorithm": "\nCreate a new algorithm that updates the distance matrix by combining the weighted sum of the normalized edge usage, the squared difference between the local and global optimum tours, and an additional randomness with parameters (0.4, 0.6, 0.3) respectively, then sorts the nodes based on both the maximum normalized edge usage and the maximum distance from the global optimum tour to determine the top nodes for perturbation.\n",
          "code": "import numpy as np\n\ndef get_matrix_and_nodes(distance_matrix, local_opt_tour, global_opt_tour, edge_n_used):\n    weighted_combined_metric = (0.4 * (edge_n_used / np.max(edge_n_used))) + (0.6 * (local_opt_tour - global_opt_tour) ** 2) + (0.3 * np.random.rand(distance_matrix.shape[0], distance_matrix.shape[1]))\n    new_distance_matrix = distance_matrix + weighted_combined_metric\n    perturb_nodes = np.argsort(np.maximum(np.max(edge_n_used, axis=1), np.max(new_distance_matrix, axis=1)))\n    \n    return new_distance_matrix, perturb_nodes",
          "objective": 0.09815,
          "other_inf": null
     }
}