{
     "parent1": {
          "algorithm": "Create a new algorithm that updates the distance matrix by incorporating a combination of the normalized edge usage, the average difference between local and global optimum tours, and the randomness, and sorts the nodes based on the maximum normalized edge usage and minimum distance from the global optimum tour to identify the top nodes for perturbation.",
          "code": "import numpy as np\n\ndef get_matrix_and_nodes(distance_matrix, local_opt_tour, global_opt_tour, edge_n_used):\n    combined_metric = (0.6 * (edge_n_used / np.max(edge_n_used))) + (0.4 * (local_opt_tour - global_opt_tour))\n    randomness = np.random.rand(distance_matrix.shape[0], distance_matrix.shape[1])\n    new_distance_matrix = distance_matrix + combined_metric + (0.1 * randomness)\n    perturb_nodes = np.argsort(np.maximum(np.max(edge_n_used, axis=1), np.min(new_distance_matrix, axis=1)))\n    \n    return new_distance_matrix, perturb_nodes",
          "objective": 0.03538,
          "other_inf": null
     },
     "parent2": {
          "algorithm": "new_distance_matrix, perturb_nodes",
          "code": "import numpy as np\n\ndef get_matrix_and_nodes(distance_matrix, local_opt_tour, global_opt_tour, edge_n_used):\n    tour_difference = (local_opt_tour - global_opt_tour) ** 2\n    average_edge_usage = np.mean(edge_n_used, axis=1)\n    weighted_factor = 0.5 \n    inverse_distance_matrix = 1 / (distance_matrix + 1)\n    \n    new_distance_matrix = distance_matrix + (tour_difference + average_edge_usage * np.random.rand(distance_matrix.shape[0]) + inverse_distance_matrix) * weighted_factor\n    perturb_nodes = np.argsort(np.maximum(np.max(edge_n_used, axis=1), np.max(new_distance_matrix, axis=1)))\n    \n    return new_distance_matrix, perturb_nodes",
          "objective": 0.04594,
          "other_inf": null
     },
     "parent3": {
          "algorithm": "Create a new algorithm that updates the distance matrix by incorporating a combination of the normalized edge usage, the average difference between local and global optimum tours, and the randomness, and sorts the nodes based on the maximum normalized edge usage and minimum distance from the global optimum tour to identify the top nodes for perturbation.",
          "code": "import numpy as np\n\ndef get_matrix_and_nodes(distance_matrix, local_opt_tour, global_opt_tour, edge_n_used):\n    combined_metric = (0.6 * (edge_n_used / np.max(edge_n_used))) + (0.4 * (local_opt_tour - global_opt_tour))\n    randomness = np.random.rand(distance_matrix.shape[0], distance_matrix.shape[1])\n    new_distance_matrix = distance_matrix + combined_metric + (0.1 * randomness)\n    perturb_nodes = np.argsort(np.maximum(np.max(edge_n_used, axis=1), np.min(new_distance_matrix, axis=1)))\n    \n    return new_distance_matrix, perturb_nodes",
          "objective": 0.03538,
          "other_inf": null
     },
     "parent4": {
          "algorithm": "new_distance_matrix, perturb_nodes",
          "code": "import numpy as np\n\ndef get_matrix_and_nodes(distance_matrix, local_opt_tour, global_opt_tour, edge_n_used):\n    tour_difference = (local_opt_tour - global_opt_tour) ** 2\n    average_edge_usage = np.mean(edge_n_used, axis=1)\n    weighted_factor = 0.5 \n    inverse_distance_matrix = 1 / (distance_matrix + 1)\n    \n    new_distance_matrix = distance_matrix + (tour_difference + average_edge_usage * np.random.rand(distance_matrix.shape[0]) + inverse_distance_matrix) * weighted_factor\n    perturb_nodes = np.argsort(np.maximum(np.max(edge_n_used, axis=1), np.max(new_distance_matrix, axis=1)))\n    \n    return new_distance_matrix, perturb_nodes",
          "objective": 0.04594,
          "other_inf": null
     },
     "parent5": {
          "algorithm": "Create a new algorithm that updates the distance matrix by incorporating a combination of the normalized edge usage, the average difference between local and global optimum tours, and the randomness, and sorts the nodes based on the maximum normalized edge usage and minimum distance from the global optimum tour to identify the top nodes for perturbation.",
          "code": "import numpy as np\n\ndef get_matrix_and_nodes(distance_matrix, local_opt_tour, global_opt_tour, edge_n_used):\n    combined_metric = (0.6 * (edge_n_used / np.max(edge_n_used))) + (0.4 * (local_opt_tour - global_opt_tour))\n    randomness = np.random.rand(distance_matrix.shape[0], distance_matrix.shape[1])\n    new_distance_matrix = distance_matrix + combined_metric + (0.1 * randomness)\n    perturb_nodes = np.argsort(np.maximum(np.max(edge_n_used, axis=1), np.min(new_distance_matrix, axis=1)))\n    \n    return new_distance_matrix, perturb_nodes",
          "objective": 0.03538,
          "other_inf": null
     },
     "offspring": {
          "algorithm": "Create a new algorithm that updates the distance matrix by adjusting the edge weights based on the average edge usage and the distance from the global optimum tour, and sorts the nodes based on the minimum distance from the global optimum tour and maximum edge usage to identify the top nodes for perturbation.",
          "code": "import numpy as np\n\ndef get_matrix_and_nodes(distance_matrix, local_opt_tour, global_opt_tour, edge_n_used):\n    average_edge_usage = np.mean(edge_n_used, axis=1)\n    inverse_distance_matrix = 1 / (distance_matrix + 1)\n    weight_factor = 0.3\n    \n    new_distance_matrix = distance_matrix + (average_edge_usage * inverse_distance_matrix) * weight_factor\n    perturb_nodes = np.argsort(np.maximum(np.min(new_distance_matrix, axis=1), np.max(edge_n_used, axis=1)))\n    \n    return new_distance_matrix, perturb_nodes",
          "objective": 0.18983,
          "other_inf": null
     }
}